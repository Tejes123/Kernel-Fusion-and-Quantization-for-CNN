{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e06cf1",
   "metadata": {},
   "source": [
    "## Ohm Muruga Thunai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12de3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ecfb41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991f7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_engine(engine_file_path):\n",
    "    assert os.path.exists(engine_file_path)\n",
    "    print(\"Reading engine from file {}\".format(engine_file_path))\n",
    "    with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "        return runtime.deserialize_cuda_engine(f.read())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ca5a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_single_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\").resize((32, 32))\n",
    "    img_arr = np.array(img, dtype=np.float32)  # H×W×C\n",
    "    img_arr = img_arr.transpose(2, 0, 1)  # C×H×W\n",
    "    img_arr = img_arr[np.newaxis, ...]    # 1×C×H×W\n",
    "    return img_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5285553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # initializes CUDA\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def infer(engine_path, image_path):\n",
    "    # Load engine\n",
    "    runtime = trt.Runtime(TRT_LOGGER)\n",
    "    with open(engine_path, \"rb\") as f:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Load & preprocess image\n",
    "    # img = Image.open(image_path).convert(\"RGB\").resize((32, 32))\n",
    "    # img_arr = np.array(img, dtype=np.float32)  # H×W×C\n",
    "    # img_arr = img_arr.transpose(2, 0, 1)  # C×H×W\n",
    "    # img_arr = img_arr[np.newaxis, ...]    # 1×C×H×W\n",
    "    img_arr = pre_process_single_image(image_path)\n",
    "\n",
    "    print(img_arr.shape) # (1, 3, 32, 32) (batch, 3, height, width)\n",
    "\n",
    "    # Set dynamic input shape\n",
    "    input_name = engine.get_tensor_name(0)\n",
    "    context.set_input_shape(input_name, img_arr.shape)\n",
    "\n",
    "    assert context.all_binding_shapes_specified\n",
    "\n",
    "    # Get input/output shapes & dtypes\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    \n",
    "    stream = cuda.Stream()\n",
    "    for idx in range(engine.num_io_tensors):\n",
    "        name = engine.get_tensor_name(idx)\n",
    "        shape = context.get_tensor_shape(name)  # resolved dims\n",
    "        dtype = trt.nptype(engine.get_tensor_dtype(name))\n",
    "        size = int(np.prod(shape))\n",
    "\n",
    "        # Allocate host/device memory\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        dev_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        bindings.append(int(dev_mem))\n",
    "\n",
    "        if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n",
    "            np.copyto(host_mem, img_arr.ravel())\n",
    "        else:\n",
    "            outputs.append((name, shape, dtype, host_mem))\n",
    "\n",
    "        context.set_tensor_address(name, int(dev_mem))\n",
    "\n",
    "    img_arr = np.ascontiguousarray(img_arr.astype(np.float32))\n",
    "\n",
    "\n",
    "    # Transfer input, run, and fetch output\n",
    "    input_memory = cuda.mem_alloc(img_arr.nbytes)\n",
    "\n",
    "    cuda.memcpy_htod_async(input_memory, img_arr, stream)\n",
    "    context.execute_async_v3(stream_handle=stream.handle)\n",
    "    print(outputs)\n",
    "    cuda.memcpy_dtoh_async(outputs[0][3], bindings[1], stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    # Postprocess logits → predicted class\n",
    "    out_name, shape, dtype, host_out = outputs[0]\n",
    "    logits = np.array(host_out).reshape(shape)\n",
    "    pred_class = int(np.argmax(logits, axis=1)[0])\n",
    "    print(f\"Predicted class index: {pred_class}\")\n",
    "\n",
    "# Example call\n",
    "# :contentReference[oaicite:1]{index=1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_timimng_Muruga(engine_path, image_path):\n",
    "    # Load engine\n",
    "    runtime = trt.Runtime(TRT_LOGGER)\n",
    "    with open(engine_path, \"rb\") as f:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Preprocess the Image\n",
    "    img_arr = pre_process_single_image(image_path)\n",
    "        \n",
    "    input_name = engine.get_tensor_name(0)\n",
    "    output_name = engine.get_tensor_name(1)\n",
    "    context.set_input_shape(input_name, img_arr.shape)\n",
    "    \n",
    "    assert context.all_binding_shapes_specified\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1f48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorrt as trt\n",
    "\n",
    "def timed_infer(engine, input_image):\n",
    "    # Assume input_image is np.array (1,3,32,32), dtype=float32\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    runtime = trt.Runtime(TRT_LOGGER)\n",
    "    \n",
    "    with open(engine, \"rb\") as f:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # preprocess the image\n",
    "    input_image = pre_process_single_image(input_image)\n",
    "\n",
    "    input_name = engine.get_tensor_name(0)\n",
    "    output_name = engine.get_tensor_name(1)\n",
    "    context.set_input_shape(input_name, input_image.shape)\n",
    "\n",
    "    # Allocate memory\n",
    "    input_image = np.ascontiguousarray(input_image.astype(np.float32))\n",
    "    output_shape = context.get_tensor_shape(output_name)\n",
    "    output_size = int(np.prod(output_shape))\n",
    "    output_dtype = trt.nptype(engine.get_tensor_dtype(output_name))\n",
    "\n",
    "    device_input = cuda.mem_alloc(input_image.nbytes)\n",
    "    device_output = cuda.mem_alloc(output_size * np.dtype(output_dtype).itemsize)\n",
    "\n",
    "    context.set_tensor_address(input_name, int(device_input))\n",
    "    context.set_tensor_address(output_name, int(device_output))\n",
    "\n",
    "    # Prepare output buffer\n",
    "    host_output = np.empty(output_shape, dtype=output_dtype)\n",
    "\n",
    "    stream = cuda.Stream()\n",
    "\n",
    "    # CUDA Events for timing\n",
    "    start_h2d = cuda.Event()\n",
    "    end_h2d = cuda.Event()\n",
    "\n",
    "    start_exec = cuda.Event()\n",
    "    end_exec = cuda.Event()\n",
    "\n",
    "    start_d2h = cuda.Event()\n",
    "    end_d2h = cuda.Event()\n",
    "\n",
    "    # Host to Device\n",
    "    start_h2d.record(stream)\n",
    "    cuda.memcpy_htod_async(device_input, input_image, stream)\n",
    "    end_h2d.record(stream)\n",
    "\n",
    "    # Inference\n",
    "    start_exec.record(stream)\n",
    "    context.execute_async_v3(stream.handle)\n",
    "    end_exec.record(stream)\n",
    "\n",
    "    # Device to Host\n",
    "    start_d2h.record(stream)\n",
    "    cuda.memcpy_dtoh_async(host_output, device_output, stream)\n",
    "    end_d2h.record(stream)\n",
    "\n",
    "    # print(host_output[0])\n",
    "\n",
    "    # Synchronize\n",
    "    stream.synchronize()\n",
    "\n",
    "    # Measure times (ms)\n",
    "    h2d_time = start_h2d.time_till(end_h2d)\n",
    "    exec_time = start_exec.time_till(end_exec)\n",
    "    d2h_time = start_d2h.time_till(end_d2h)\n",
    "\n",
    "    total_time = h2d_time + exec_time + d2h_time\n",
    "\n",
    "    print(f\"H2D Time     : {h2d_time:.3f} ms\")\n",
    "    print(f\"Infer Time   : {exec_time:.3f} ms\")\n",
    "    print(f\"D2H Time     : {d2h_time:.3f} ms\")\n",
    "    print(f\"Total Time   : {total_time:.3f} ms\")\n",
    "\n",
    "    return host_output, h2d_time, exec_time, d2h_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "59ab6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorrt as trt\n",
    "\n",
    "def timed_infer_batches(engine, test_loader):\n",
    "    # Assume input_image is np.array (1,3,32,32), dtype=float32\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    runtime = trt.Runtime(TRT_LOGGER)\n",
    "    \n",
    "    with open(engine, \"rb\") as f:\n",
    "        engine = runtime.deserialize_cuda_engine(f.read())\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    outputs = []\n",
    "    total_inference_time = 0\n",
    "    total_host_to_device_time = 0\n",
    "    total_device_to_host_time = 0\n",
    "    total_time = 0\n",
    "\n",
    "    batches_seen = 0\n",
    "\n",
    "    for (input_data, label) in test_loader:\n",
    "        input_image = input_data.numpy()\n",
    "\n",
    "        # preprocess the image\n",
    "        # input_image = pre_process_single_image(input_image)\n",
    "\n",
    "        input_name = engine.get_tensor_name(0)\n",
    "        output_name = engine.get_tensor_name(1)\n",
    "        context.set_input_shape(input_name, input_image.shape)\n",
    "\n",
    "        # Allocate memory\n",
    "        input_image = np.ascontiguousarray(input_image.astype(np.float32))\n",
    "        output_shape = context.get_tensor_shape(output_name)\n",
    "        output_size = int(np.prod(output_shape))\n",
    "        output_dtype = trt.nptype(engine.get_tensor_dtype(output_name))\n",
    "\n",
    "        device_input = cuda.mem_alloc(input_image.nbytes)\n",
    "        device_output = cuda.mem_alloc(output_size * np.dtype(output_dtype).itemsize)\n",
    "\n",
    "        context.set_tensor_address(input_name, int(device_input))\n",
    "        context.set_tensor_address(output_name, int(device_output))\n",
    "\n",
    "        # Prepare output buffer\n",
    "        host_output = np.empty(output_shape, dtype=output_dtype)\n",
    "\n",
    "        stream = cuda.Stream()\n",
    "\n",
    "        # CUDA Events for timing\n",
    "        start_h2d = cuda.Event()\n",
    "        end_h2d = cuda.Event()\n",
    "\n",
    "        start_exec = cuda.Event()\n",
    "        end_exec = cuda.Event()\n",
    "\n",
    "        start_d2h = cuda.Event()\n",
    "        end_d2h = cuda.Event()\n",
    "\n",
    "        # Host to Device\n",
    "        start_h2d.record(stream)\n",
    "        cuda.memcpy_htod_async(device_input, input_image, stream)\n",
    "        end_h2d.record(stream)\n",
    "\n",
    "        # Inference\n",
    "        start_exec.record(stream)\n",
    "        context.execute_async_v3(stream.handle)\n",
    "        end_exec.record(stream)\n",
    "\n",
    "        # Device to Host\n",
    "        start_d2h.record(stream)\n",
    "        cuda.memcpy_dtoh_async(host_output, device_output, stream)\n",
    "        end_d2h.record(stream)\n",
    "\n",
    "        # Synchronize\n",
    "        stream.synchronize()\n",
    "\n",
    "        # Measure times (ms)\n",
    "        h2d_time = start_h2d.time_till(end_h2d)\n",
    "        exec_time = start_exec.time_till(end_exec)\n",
    "        d2h_time = start_d2h.time_till(end_d2h)\n",
    "\n",
    "        total_inference_time += exec_time\n",
    "        total_host_to_device_time += h2d_time\n",
    "        total_device_to_host_time += d2h_time\n",
    "\n",
    "        total_time += h2d_time + exec_time + d2h_time\n",
    "\n",
    "        outputs.append(host_output[0])\n",
    "        batches_seen += 1\n",
    "    \n",
    "    # Calulate the avegare time\n",
    "    avg_inference_time_trt = total_inference_time / batches_seen\n",
    "    avg_host_to_device_time = total_host_to_device_time / batches_seen\n",
    "    avg_device_to_host_time = total_device_to_host_time / batches_seen\n",
    "    avg_time_per_batch = total_time / batches_seen\n",
    "\n",
    "    print(f\"H2D Time per batch    : {avg_host_to_device_time:.3f} ms\")\n",
    "    print(f\"Infer Time per batch  : {avg_inference_time_trt:.3f} ms\")\n",
    "    print(f\"D2H Time per batch    : {avg_device_to_host_time:.3f} ms\")\n",
    "    print(f\"Total Time per batch : {avg_time_per_batch:.3f} ms\")\n",
    "\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd081157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5074.078    -2192.8315    1501.8422   -5104.639   -10178.127\n",
      " -21113.158    -2229.9111   -5574.8438      50.58682  15115.169  ]\n",
      "H2D Time     : 0.051 ms\n",
      "Infer Time   : 2.814 ms\n",
      "D2H Time     : 0.167 ms\n",
      "Total Time   : 3.032 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[  5074.078  ,  -2192.8315 ,   1501.8422 ,  -5104.639  ,\n",
       "         -10178.127  , -21113.158  ,  -2229.9111 ,  -5574.8438 ,\n",
       "             50.58682,  15115.169  ]], dtype=float32),\n",
       " 0.05119999870657921,\n",
       " 2.813983917236328,\n",
       " 0.1666879951953888)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine_path = r\"resnet50_new.engine\"\n",
    "image_path = r\"C:\\Tejeswar\\Fusion\\sampleImages\\cifar10_image_2_label_9_truck.jpg\"\n",
    "\n",
    "# timed_infer(engine_path, image_path)\n",
    "# inferMuruga(engine_path, image_path)\n",
    "\n",
    "timed_infer_batches(engine_path, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "murugan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
